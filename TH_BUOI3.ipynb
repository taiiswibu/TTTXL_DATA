{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\Python\\TH_TXLDL\\TH_BUOI3.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/Python/TH_TXLDL/TH_BUOI3.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/Python/TH_TXLDL/TH_BUOI3.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m  \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/Python/TH_TXLDL/TH_BUOI3.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m  \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4  import BeautifulSoup\n",
    "import pandas  as pd\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot  as plt \n",
    "import seaborn  as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_urls = ['https://notes.ayushsharma.in/technology',                \n",
    "             'https://www.ayush.nz/video-games'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "seed_urls = [\n",
    "    'https://notes.ayushsharma.in/technology',\n",
    "    'https://www.ayush.nz/video-games'\n",
    "]\n",
    "\n",
    "def build_dataset(seed_urls):\n",
    "    news_data = []\n",
    "    for url in seed_urls:\n",
    "        news_category = url.split('/')[-1]\n",
    "        data = requests.get(url)\n",
    "        soup = BeautifulSoup(data.content, 'html.parser')\n",
    "        tables = soup.find_all('div',class_= 'col')\n",
    "        for id, i in enumerate(tables):\n",
    "            title = i.find_all('h5',class_='card-title')\n",
    "            excerpt = i.find_all('small', class_ = 'card-text')\n",
    "            pub_date = i.find_all('div', class_ = 'card-footer text-end')\n",
    "            if title and excerpt and pub_date:\n",
    "                title_position = title[0].text.strip()\n",
    "                excerpt_position = excerpt[0].text.strip()\n",
    "                date_published = pub_date[0].text.strip()\n",
    "                news_data.append([title_position, excerpt_position, date_published, news_category])\n",
    "        df = pd.DataFrame(news_data,columns=['title', 'excerpt', 'pub_date', 'category'])\n",
    "    return df\n",
    "\n",
    "# Gọi hàm để lấy dữ liệu\n",
    "df = build_dataset(seed_urls)\n",
    "\n",
    "\n",
    "# Lưu dữ liệu thành file CSV\n",
    "df.to_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get('https://notes.ayushsharma.in/technology')\n",
    "soup = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "soup.find_all('h5','card-title')\n",
    "soup.find_all('small','card-text')\n",
    "soup.find_all(class_='card-footer text_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được ghi vào file CSV: dataset1.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Tạo danh sách trống để chứa dữ liệu\n",
    "data = []\n",
    "\n",
    "# Lặp qua các URL và lấy dữ liệu từ mỗi URL\n",
    "for url in seed_urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Lấy dữ liệu từ trang web\n",
    "    titles = soup.find_all('h5', {'class': 'card-title'})\n",
    "    excerpts = soup.find_all('small', {'class': 'card-text'})\n",
    "    pub_dates = soup.find_all(class_='card-footer text-end')\n",
    "    \n",
    "    # Thêm dữ liệu vào danh sách data\n",
    "    data.append({\n",
    "        'titles': [title.text for title in titles],\n",
    "        'excerpts': [excerpt.text for excerpt in excerpts],\n",
    "        'pub_dates': [date.text for date in pub_dates]\n",
    "    })\n",
    "    \n",
    "    for item in data:\n",
    "        for i in range(len(item['titles'])):\n",
    "            print(f\"Title: {item['titles'][i]}\")\n",
    "            print(f\"Excerpt: {item['excerpts'][i]}\")\n",
    "            print(f\"pub_date: {item['pub_dates'][i]}\")\n",
    "            print()\n",
    "            \n",
    "# Ghi dữ liệu vào file CSV\n",
    "csv_file_path = 'dataset1.csv'\n",
    "\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Title', 'Excerpt', 'Pub_Date']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # Viết header\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Viết dữ liệu\n",
    "    for item in data:\n",
    "        for i in range(len(item['titles'])):\n",
    "            writer.writerow({\n",
    "                'Title': item['titles'][i],\n",
    "                'Excerpt': item['excerpts'][i],\n",
    "                'Pub_Date': item['pub_dates'][i]\n",
    "            })\n",
    "\n",
    "print(f\"Dữ liệu đã được ghi vào file CSV: {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vantai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "stopword_list.remove('no')\n",
    "\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2:Remove HTML tags \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags (text):\n",
    "    soup= BeautifulSoup (text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Loại bỏ ký tự có dấu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_accented_chars(text): \n",
    "    text =unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'Ignore') \n",
    "    return text\n",
    "\n",
    "remove_accented_chars('Sómě Accentěd text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You all cannot expand contractions I would think\n"
     ]
    }
   ],
   "source": [
    "CONTRACTION_MAP = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"Y'all\": \"You all\"\n",
    "    \n",
    "}\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "                                \n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)  # Loại bỏ dấu nháy đơn nếu cần thiết\n",
    "    return expanded_text\n",
    "\n",
    "result = expand_contractions(\"Y'all can't expand contractions I'd think\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You all cannot expand contractions I would think\n"
     ]
    }
   ],
   "source": [
    "from contractions_utils import expand_contractions\n",
    "result = expand_contractions(\"Y'all can't expand contractions I'd think\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sub() missing 1 required positional argument: 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\Python\\TH_TXLDL\\TH_BUOI3.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/Python/TH_TXLDL/TH_BUOI3.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     text\u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(pattern, text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/Python/TH_TXLDL/TH_BUOI3.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m text\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/Python/TH_TXLDL/TH_BUOI3.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m remove_special_characters (\u001b[39m\"\u001b[39;49m\u001b[39mWell this was fun! What do you think? 123@!\u001b[39;49m\u001b[39m\"\u001b[39;49m, remove_digits\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32md:\\Code\\Python\\TH_TXLDL\\TH_BUOI3.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/Python/TH_TXLDL/TH_BUOI3.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremove_special_characters\u001b[39m (text, remove_digits\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m): \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/Python/TH_TXLDL/TH_BUOI3.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     pattern \u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[a-zA-z0-9\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m remove_digits \u001b[39melse\u001b[39;00m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[a-zA-z\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/Python/TH_TXLDL/TH_BUOI3.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     text\u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49msub(pattern, text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/Python/TH_TXLDL/TH_BUOI3.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m text\n",
      "\u001b[1;31mTypeError\u001b[0m: sub() missing 1 required positional argument: 'string'"
     ]
    }
   ],
   "source": [
    "def remove_special_characters (text, remove_digits=False): \n",
    "    pattern =r'[a-zA-z0-9\\s]' if not remove_digits else r'[a-zA-z\\s]'\n",
    "    text= re.sub(pattern, text)\n",
    "    return text\n",
    "\n",
    "remove_special_characters (\"Well this was fun! What do you think? 123@!\", remove_digits=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
